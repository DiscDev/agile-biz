# Model Status

View current multi-LLM configuration, connectivity status, and usage statistics.

## Usage
```
/model-status
```

## What It Shows

This command displays comprehensive information about your multi-LLM setup:
- Active providers and their status
- Current routing configuration
- Usage statistics and costs
- Performance metrics
- Error logs and issues

## Output Sections

### 1. Provider Status
Shows which LLM providers are configured and their connectivity:
```
ğŸ“Š LLM Provider Status
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Claude     [Connected]  claude-3-haiku (primary for: code)
âœ… Gemini     [Connected]  gemini-1.5-flash (primary for: analysis, general)
âœ… Perplexity [Connected]  sonar (primary for: research)
âš ï¸ OpenAI     [No API Key] Not configured
```

### 2. Routing Configuration
Displays how tasks are routed to different models:
```
ğŸ”„ Intelligent Routing
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Research:     Perplexity â†’ Gemini â†’ Claude
Code Gen:     Claude â†’ Gemini
Analysis:     Gemini â†’ Claude
General:      Gemini â†’ Claude
Parallel:     Max 5 concurrent requests
```

### 3. Usage Statistics
Shows current session and project usage:
```
ğŸ’° Usage & Costs
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Session:      $1.23 / $5.00 (24.6%)
Project:      $12.45 / $50.00 (24.9%)
Tokens Used:  145,234 (input) / 89,123 (output)
API Calls:    Claude: 45, Gemini: 123, Perplexity: 18
Savings:      $35.67 (74% vs Claude-only)
```

### 4. Performance Metrics
Displays speed and reliability metrics:
```
âš¡ Performance
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Avg Response: 1.2s (vs 3.8s sequential)
Parallel Ops: 89 successful, 2 fallbacks
Cache Hits:   234/567 (41.3%)
Error Rate:   0.3%
```

### 5. Recent Activity
Shows last few model interactions:
```
ğŸ“ Recent Activity
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[10:23:15] Research Agent â†’ Perplexity (market analysis) âœ… 1.3s
[10:23:45] Coder Agent â†’ Claude (authentication.js) âœ… 0.8s
[10:24:02] Analysis Agent â†’ Gemini (performance review) âœ… 1.1s
[10:24:15] Research Agent â†’ Perplexity (competitor scan) âš ï¸ 3.2s â†’ Gemini fallback
```

## Command Options

### Check Specific Provider
```
/model-status --provider=gemini
```
Shows detailed information for a specific provider.

### Test Connectivity
```
/model-status --test
```
Runs connectivity tests for all configured providers.

### Show Errors
```
/model-status --errors
```
Displays recent errors and fallback events.

### Export Stats
```
/model-status --export
```
Saves detailed statistics to `machine-data/llm-stats.json`.

## Example Output

```
You: /model-status

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              Multi-LLM System Status
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š Providers (3/4 active)
  âœ… Claude     v3-haiku    [12ms ping]  $0.43 used
  âœ… Gemini     1.5-flash   [8ms ping]   $0.67 used  
  âœ… Perplexity sonar       [15ms ping]  $0.13 used
  âš ï¸ OpenAI     not configured

ğŸ”„ Routing: Intelligent (with fallback)
  â€¢ 78% requests using primary model
  â€¢ 19% using fallback (cost optimized)
  â€¢ 3% using Claude (final fallback)

ğŸ’° Cost Savings: $2.87 this session (71% reduction)
  
âš¡ Performance: 3.2x faster than sequential
  â€¢ 145 parallel operations
  â€¢ 1.3s average response time
  â€¢ 99.7% success rate

ğŸ¯ Optimization Suggestions:
  â€¢ Enable OpenAI for additional redundancy
  â€¢ Consider upgrading to Perplexity Pro for deeper research
  â€¢ Cache hit rate could be improved (currently 41%)

Use /configure-models to adjust settings
```

## Status Indicators

- âœ… **Connected** - Provider is working normally
- âš ï¸ **Degraded** - Provider responding slowly or with errors
- âŒ **Offline** - Provider not responding
- ğŸ”„ **Fallback Active** - Using backup provider
- ğŸ’¤ **Idle** - No recent requests to this provider

## Troubleshooting

### Provider Shows Offline
1. Check API key in .env file
2. Run `/model-status --test` to diagnose
3. Verify network connectivity
4. Check provider's status page

### High Error Rate
1. Review error logs with `/model-status --errors`
2. Check rate limits for each provider
3. Consider adjusting timeout values
4. Enable more aggressive caching

### Poor Performance
1. Check parallel execution limits
2. Review routing configuration
3. Consider provider latency in your region
4. Optimize task distribution

## Integration with Dashboard

The model status is also available in the real-time dashboard:
```
http://localhost:3001
# Click on "ğŸ¤– Models" tab
```

The dashboard provides:
- Real-time provider status
- Live cost tracking
- Performance graphs
- Error monitoring
- Usage trends

## Related Commands

- `/configure-models` - Set up or modify LLM configuration
- `/research-boost` - Enable enhanced research mode
- `/cost` - Detailed cost breakdown

## Notes

- Status updates every 30 seconds
- Historical data retained for 7 days
- Costs are estimates based on token usage
- Actual bills may vary slightly from estimates